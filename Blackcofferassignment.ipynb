{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishabh181/dataextraction_dataanalysi/blob/main/Blackcofferassignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H2mp1SFMaKz",
        "outputId": "7811a1e7-7fa4-438f-db47-9fe0686758db"
      },
      "id": "8H2mp1SFMaKz",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.27.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9b727943-2f24-4c6b-8c99-029283fc7a07",
      "metadata": {
        "id": "9b727943-2f24-4c6b-8c99-029283fc7a07"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "\n",
        "# Updated imports for Chrome\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
        "from selenium import webdriver\n",
        "#from selenium.webdriver import Chrome  # Use 'webdriver' for Chrome\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a35212d3-267f-40ad-a0aa-fc9fd0bf1eff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a35212d3-267f-40ad-a0aa-fc9fd0bf1eff",
        "outputId": "e982b16a-4214-44f1-d024-d529bd89987c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "97255b3a-1e1f-459e-8a2c-ee3cf7a61595",
      "metadata": {
        "id": "97255b3a-1e1f-459e-8a2c-ee3cf7a61595"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "STOPWORDS_PATH = \"Blackcoffer/StopWords/\"\n",
        "MASTER_DICT_PATH = \"Blackcoffer/MasterDictionary/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f6c1c62e-e0b4-4be7-bb2a-b719ab3e875c",
      "metadata": {
        "id": "f6c1c62e-e0b4-4be7-bb2a-b719ab3e875c"
      },
      "outputs": [],
      "source": [
        "# Load Stop Words\n",
        "def load_stop_words():\n",
        "    stop_words = set()\n",
        "    for file_name in os.listdir(STOPWORDS_PATH):\n",
        "        try:\n",
        "            with open(os.path.join(STOPWORDS_PATH, file_name), 'r', encoding='utf-8') as f:\n",
        "                stop_words.update(f.read().splitlines())\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                with open(os.path.join(STOPWORDS_PATH, file_name), 'r', encoding='latin-1') as f:  # Or 'cp1252'\n",
        "                    stop_words.update(f.read().splitlines())\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Warning: Could not decode file {file_name} with either UTF-8 or Latin-1 encoding.\")\n",
        "    return stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "62825872-7083-4b7d-a89e-10af2ab3d0f9",
      "metadata": {
        "id": "62825872-7083-4b7d-a89e-10af2ab3d0f9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Load Positive and Negative Words\n",
        "# def load_master_dict():\n",
        "#     positive_words = set()\n",
        "#     negative_words = set()\n",
        "#     with open(os.path.join(MASTER_DICT_PATH, \"positive-words.txt\"), 'r', encoding='latin-1') as f:\n",
        "#         positive_words.update(f.read().splitlines())\n",
        "#     with open(os.path.join(MASTER_DICT_PATH, \"negative-words.txt\"), 'r',  encoding='latin-1') as f:\n",
        "#         negative_words.update(f.read().splitlines())\n",
        "#     return positive_words, negative_words\n",
        "\n",
        "def load_master_dict():\n",
        "    positive_words = set()\n",
        "    negative_words = set()\n",
        "\n",
        "    for file_path, word_set in [(os.path.join(MASTER_DICT_PATH, \"positive-words.txt\"), positive_words),\n",
        "                               (os.path.join(MASTER_DICT_PATH, \"negative-words.txt\"), negative_words)]:\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                word_set.update(f.read().splitlines())\n",
        "        except UnicodeDecodeError:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                    word_set.update(f.read().splitlines())\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Warning: Could not decode file {file_path} with either UTF-8 or Latin-1 encoding.\")\n",
        "\n",
        "    return positive_words, negative_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f084c37a-d669-4d06-8744-a7bd600d04cf",
      "metadata": {
        "id": "f084c37a-d669-4d06-8744-a7bd600d04cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "897987cc-adde-4bbf-8ab2-9a2107da6511"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Blackcoffer/StopWords/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1c523542a19e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSTOP_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mPOSITIVE_WORDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNEGATIVE_WORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_master_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-1c97feb81cca>\u001b[0m in \u001b[0;36mload_stop_words\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOPWORDS_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOPWORDS_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Blackcoffer/StopWords/'"
          ]
        }
      ],
      "source": [
        "STOP_WORDS = load_stop_words()\n",
        "POSITIVE_WORDS, NEGATIVE_WORDS = load_master_dict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A9PoEyLpLQaP"
      },
      "id": "A9PoEyLpLQaP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(os.path.join(MASTER_DICT_PATH, \"negative-words.txt\"), 'r',  encoding='latin-1')\n",
        "print(f.read())"
      ],
      "metadata": {
        "id": "J8W4TOZCeiiP"
      },
      "id": "J8W4TOZCeiiP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcRNiHibv9sr"
      },
      "id": "xcRNiHibv9sr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa35fd8-73cd-4d01-a174-633aa3f82173",
      "metadata": {
        "id": "9aa35fd8-73cd-4d01-a174-633aa3f82173"
      },
      "outputs": [],
      "source": [
        "# Clean and tokenize text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return [word for word in tokens if word not in STOP_WORDS and word not in punctuation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f8b9453-134b-4b83-ae4f-962fa1e7004a",
      "metadata": {
        "id": "1f8b9453-134b-4b83-ae4f-962fa1e7004a"
      },
      "outputs": [],
      "source": [
        "# Calculate variables\n",
        "def calculate_scores(tokens, text):\n",
        "    positive_score = sum(1 for word in tokens if word in POSITIVE_WORDS)\n",
        "    negative_score = sum(1 for word in tokens if word in NEGATIVE_WORDS)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    avg_sentence_length = len(tokens) / len(sentences)\n",
        "\n",
        "    complex_words = [word for word in tokens if count_syllables(word) > 2]\n",
        "    percentage_complex_words = len(complex_words) / len(tokens)\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "    avg_words_per_sentence = len(tokens) / len(sentences)\n",
        "    syllables_per_word = sum(count_syllables(word) for word in tokens) / len(tokens)\n",
        "\n",
        "    personal_pronouns = len(re.findall(r\"\\b(I|we|my|ours|us)\\b\", text, re.IGNORECASE))\n",
        "    avg_word_length = sum(len(word) for word in tokens) / len(tokens)\n",
        "\n",
        "    return {\n",
        "        \"Positive Score\": positive_score,\n",
        "        \"Negative Score\": negative_score,\n",
        "        \"Polarity Score\": polarity_score,\n",
        "        \"Subjectivity Score\": subjectivity_score,\n",
        "        \"Avg Sentence Length\": avg_sentence_length,\n",
        "        \"Percentage of Complex Words\": percentage_complex_words,\n",
        "        \"Fog Index\": fog_index,\n",
        "        \"Avg Number of Words Per Sentence\": avg_words_per_sentence,\n",
        "        \"Complex Word Count\": len(complex_words),\n",
        "        \"Word Count\": len(tokens),\n",
        "        \"Syllable Per Word\": syllables_per_word,\n",
        "        \"Personal Pronouns\": personal_pronouns,\n",
        "        \"Avg Word Length\": avg_word_length,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943561eb-8c15-43ab-838a-285a5293601a",
      "metadata": {
        "id": "943561eb-8c15-43ab-838a-285a5293601a"
      },
      "outputs": [],
      "source": [
        "# Count syllables in a word\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiou\"\n",
        "    count = sum(1 for char in word if char in vowels)\n",
        "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
        "        count -= 1\n",
        "    return max(1, count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b78a8b6-1e78-4618-8573-e112f723da71",
      "metadata": {
        "id": "0b78a8b6-1e78-4618-8573-e112f723da71"
      },
      "outputs": [],
      "source": [
        "# Extract article text using Selenium\n",
        "def extract_text_from_url(url):\n",
        "    options = EdgeOptions()\n",
        "    options.add_argument(\"--headless\")  # Run Edge in headless mode\n",
        "    options.use_chromium = True\n",
        "    service = EdgeService('path_to_msedge_driver')  # Provide the full path to your Edge WebDriver\n",
        "    driver = Edge(service=service, options=options)\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        title = soup.find('h1').get_text(strip=True)\n",
        "        paragraphs = soup.find_all('p')\n",
        "        article_text = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
        "        return title, article_text\n",
        "    finally:\n",
        "        driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b896c754-ee1d-4d21-adeb-3a61cfe30f99",
      "metadata": {
        "id": "b896c754-ee1d-4d21-adeb-3a61cfe30f99"
      },
      "outputs": [],
      "source": [
        "# Main Function\n",
        "def main():\n",
        "    # Load input data\n",
        "    input_df = pd.read_excel(\"Blackcoffer/Input.xlsx\")\n",
        "\n",
        "    output_data = []\n",
        "    for index, row in input_df.iterrows():\n",
        "        url_id = row['URL_ID']\n",
        "        url = row['URL']\n",
        "\n",
        "        try:\n",
        "            title, article_text = extract_text_from_url(url)\n",
        "            tokens = clean_text(article_text)\n",
        "            scores = calculate_scores(tokens, article_text)\n",
        "\n",
        "            output_row = {\n",
        "                \"URL_ID\": url_id,\n",
        "                \"URL\": url,\n",
        "                **scores\n",
        "            }\n",
        "            output_data.append(output_row)\n",
        "\n",
        "            # Save article text to file\n",
        "            os.makedirs(\"articles\", exist_ok=True)\n",
        "            with open(f\"articles/{url_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(title + \"\\n\" + article_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing URL_ID {url_id}: {e}\")\n",
        "\n",
        "    # Save output data\n",
        "    output_df = pd.DataFrame(output_data)\n",
        "    output_df.to_excel(\"Output Data Structure.xlsx\", index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17614029-3311-4f1c-b98d-af8f97d369d5",
      "metadata": {
        "id": "17614029-3311-4f1c-b98d-af8f97d369d5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}